{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¹…ì¿¼ë¦¬ì—ì„œ ë°ì´í„°ë¥¼ ì •ì œí•˜ì—¬ ì“¸ ìˆ˜ ìˆëŠ” ë°ì´í„°ë“¤ë§Œ ë‚¨ê¹ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import auth\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. í™˜ê²½ ì„¤ì • ë° êµ¬ê¸€ ì¸ì¦\n",
    "# ==============================================================================\n",
    "# 1) êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 2) BigQuery ì¸ì¦ (íŒì—… ë¡œê·¸ì¸ í•„ìš”)\n",
    "print(\"ğŸ”‘ Google Cloud ì¸ì¦ ì¤‘...\")\n",
    "auth.authenticate_user()\n",
    "PROJECT_ID = 'YOUR_PROJECT_ID_HERE'\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "BASE_DIR = '/content/drive/MyDrive/REM_project/github/dataset/'\n",
    "SAVE_DIR = '/content/drive/MyDrive/REM_project/github/dataset/split_data/'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì •ì˜\n",
    "PATH_TRIAGE = BASE_DIR + 'triage.csv'\n",
    "PATH_EDSTAYS = BASE_DIR + 'edstays.csv'\n",
    "PATH_PATIENTS = BASE_DIR + 'patients.csv'\n",
    "PATH_META = BASE_DIR + 'mimic-cxr-2.0.0-metadata.csv'\n",
    "PATH_CHEXPERT = BASE_DIR + 'mimic-cxr-2.0.0-chexpert.csv'\n",
    "\n",
    "print(\"ğŸš€ [Step 1] ë°ì´í„° ì •ì œ ë° BigQuery ì—°ë™ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...\")\n",
    "\n",
    "try:\n",
    "    # ==============================================================================\n",
    "    # 2. ê¸°ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "    # ==============================================================================\n",
    "    print(\"\\nğŸ“¥ 1. CSV íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "    df_triage = pd.read_csv(PATH_TRIAGE)\n",
    "    df_edstays = pd.read_csv(PATH_EDSTAYS, usecols=['subject_id', 'stay_id', 'intime', 'outtime'])\n",
    "    df_patients = pd.read_csv(PATH_PATIENTS, usecols=['subject_id', 'gender', 'anchor_age'])\n",
    "    df_meta = pd.read_csv(PATH_META, usecols=['dicom_id', 'subject_id', 'study_id', 'StudyDate', 'StudyTime', 'ViewPosition'])\n",
    "    df_chexpert = pd.read_csv(PATH_CHEXPERT)\n",
    "\n",
    "    print(f\"   - Triage: {len(df_triage)} rows\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 3. ì‘ê¸‰ì‹¤ ë°ì´í„° ë³‘í•© (Base DataFrame ìƒì„±)\n",
    "    # ==============================================================================\n",
    "    print(\"\\nâ³ 2. ì‘ê¸‰ì‹¤ ë°ì´í„° ë³‘í•© ë° ì‹œê°„ í¬ë§· ë³€í™˜ ì¤‘...\")\n",
    "\n",
    "    # ì—¬ê¸°ì„œ df_ed_fullì´ ìµœì´ˆ ìƒì„±ë©ë‹ˆë‹¤.\n",
    "    df_ed_full = pd.merge(df_triage, df_edstays, on=['subject_id', 'stay_id'], how='inner')\n",
    "\n",
    "    df_ed_full['intime'] = pd.to_datetime(df_ed_full['intime'])\n",
    "    df_ed_full['outtime'] = pd.to_datetime(df_ed_full['outtime'])\n",
    "\n",
    "    # ==============================================================================\n",
    "    # [NEW] 3.5. BigQuery Lab ë°ì´í„° (WBC, BNP) ë³‘í•© ë¡œì§ ì‚½ì…\n",
    "    # ==============================================================================\n",
    "    print(\"\\nğŸš€ 2.5. BigQueryì—ì„œ Lab ë°ì´í„°(WBC, BNP) ì¶”ì¶œ ì¤‘...\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT subject_id, charttime, itemid, valuenum\n",
    "    FROM `physionet-data.mimiciv_3_1_hosp.labevents`\n",
    "    WHERE itemid IN (51301, 50963) AND valuenum IS NOT NULL\n",
    "    \"\"\"\n",
    "    df_lab = client.query(query).to_dataframe()\n",
    "    df_lab['charttime'] = pd.to_datetime(df_lab['charttime'])\n",
    "\n",
    "    # Lab ë°ì´í„° ë³‘í•©ì„ ìœ„í•œ ì„ì‹œ í…Œì´ë¸”\n",
    "    df_ed_subset = df_ed_full[['subject_id', 'stay_id', 'intime']].copy()\n",
    "    df_merged_lab = pd.merge(df_ed_subset, df_lab, on='subject_id', how='inner')\n",
    "\n",
    "    # ì‹œê°„ í•„í„°ë§ (Â±24ì‹œê°„)\n",
    "    df_merged_lab['time_diff'] = (df_merged_lab['charttime'] - df_merged_lab['intime']).abs()\n",
    "    df_merged_lab = df_merged_lab[df_merged_lab['time_diff'] <= pd.Timedelta(hours=24)].copy()\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±° (ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ ì„ íƒ)\n",
    "    df_merged_lab.sort_values('time_diff', inplace=True)\n",
    "    df_merged_lab.drop_duplicates(subset=['stay_id', 'itemid'], keep='first', inplace=True)\n",
    "\n",
    "    # Pivot (Long -> Wide)\n",
    "    df_lab_pivot = df_merged_lab.pivot(index='stay_id', columns='itemid', values='valuenum')\n",
    "    df_lab_pivot.rename(columns={51301: 'WBC', 50963: 'BNP'}, inplace=True)\n",
    "\n",
    "    # ë©”ì¸ ë°ì´í„°(df_ed_full)ì— ë³‘í•©\n",
    "    df_ed_full = pd.merge(df_ed_full, df_lab_pivot, on='stay_id', how='left')\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ -1 ì²˜ë¦¬\n",
    "    for col in ['WBC', 'BNP']:\n",
    "        if col not in df_ed_full.columns:\n",
    "            df_ed_full[col] = -1\n",
    "        else:\n",
    "            df_ed_full[col] = df_ed_full[col].fillna(-1)\n",
    "\n",
    "    print(f\"   âœ… Lab ë°ì´í„° ë³‘í•© ì™„ë£Œ! (WBC ê²°ì¸¡ ë¹„ìœ¨: {(df_ed_full['WBC'] == -1).mean()*100:.1f}%)\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 4. ì´ë¯¸ì§€ ì‹œì  ë§¤ì¹­ ë° ì •ì œ (ê¸°ì¡´ ë¡œì§ ê³„ì†)\n",
    "    # ==============================================================================\n",
    "    # CXR ì‹œê°„ íŒŒì‹±\n",
    "    df_meta['StudyTime'] = df_meta['StudyTime'].astype(str).str.split('.').str[0].str.zfill(6)\n",
    "    df_meta['StudyDateTime'] = pd.to_datetime(\n",
    "        df_meta['StudyDate'].astype(str) + ' ' + df_meta['StudyTime'],\n",
    "        format='%Y%m%d %H%M%S', errors='coerce'\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ”— 3. ì—‘ìŠ¤ë ˆì´ vs ì‘ê¸‰ì‹¤ ì²´ë¥˜ ì‹œê°„ ëŒ€ì¡°...\")\n",
    "    # ì´ì œ df_ed_fullì—ëŠ” WBC, BNPê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.\n",
    "    df_merged = pd.merge(df_ed_full, df_meta, on='subject_id', how='inner')\n",
    "\n",
    "    time_mask = (\n",
    "        (df_merged['StudyDateTime'] >= df_merged['intime'] - pd.Timedelta(hours=2)) &\n",
    "        (df_merged['StudyDateTime'] <= df_merged['outtime'] + pd.Timedelta(hours=2))\n",
    "    )\n",
    "    df_matched = df_merged[time_mask].copy()\n",
    "\n",
    "    # ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬\n",
    "    print(\"\\nğŸ§¹ 4. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ (Cleaning)...\")\n",
    "    df_matched = df_matched[df_matched['ViewPosition'].isin(['AP', 'PA'])].copy()\n",
    "\n",
    "    # ë°”ì´íƒˆ ê²°ì¸¡ ì œê±° (WBC, BNPëŠ” -1ë¡œ ì±„ì› ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ì‚­ì œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)\n",
    "    vital_cols = ['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp']\n",
    "    df_matched.dropna(subset=vital_cols, inplace=True)\n",
    "\n",
    "    # í™”ì”¨ -> ì„­ì”¨ ë³€í™˜\n",
    "    mask_fahrenheit = df_matched['temperature'] > 50\n",
    "    df_matched.loc[mask_fahrenheit, 'temperature'] = (df_matched.loc[mask_fahrenheit, 'temperature'] - 32) * 5.0/9.0\n",
    "\n",
    "    # ì´ìƒì¹˜ ì œê±°\n",
    "    clean_mask = (\n",
    "        (df_matched['heartrate'] > 20) & (df_matched['heartrate'] < 300) &\n",
    "        (df_matched['o2sat'] > 0) & (df_matched['o2sat'] <= 100) &\n",
    "        (df_matched['sbp'] > 30) & (df_matched['sbp'] < 300) &\n",
    "        (df_matched['dbp'] > 10) & (df_matched['dbp'] < 200) &\n",
    "        (df_matched['temperature'] > 10) & (df_matched['temperature'] < 45) &\n",
    "        (df_matched['resprate'] > 0) & (df_matched['resprate'] < 100)\n",
    "    )\n",
    "    df_cleaned = df_matched[clean_mask].copy()\n",
    "\n",
    "    # í™˜ì ì •ë³´ ë° ë¼ë²¨ ë³‘í•©\n",
    "    df_final = pd.merge(df_cleaned, df_patients, on='subject_id', how='left')\n",
    "    df_final = df_final[df_final['gender'].isin(['M', 'F'])]\n",
    "    df_final['gender'] = df_final['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "    df_final.dropna(subset=['anchor_age'], inplace=True)\n",
    "\n",
    "    # ë‚˜ì´ ì •ê·œí™”\n",
    "    age_mean = df_final['anchor_age'].mean()\n",
    "    age_std = df_final['anchor_age'].std()\n",
    "    df_final['anchor_age'] = (df_final['anchor_age'] - age_mean) / age_std\n",
    "\n",
    "    # ë¼ë²¨ ë³‘í•©\n",
    "    df_final = pd.merge(df_final, df_chexpert, on=['subject_id', 'study_id'], how='left')\n",
    "    target_diseases = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion', 'Pneumonia']\n",
    "    for col in target_diseases:\n",
    "        if col in df_final.columns:\n",
    "            df_final[col] = df_final[col].fillna(0).replace(-1, 1)\n",
    "\n",
    "    # ì´ë¯¸ì§€ ê²½ë¡œ ìƒì„±\n",
    "    def get_img_path(row):\n",
    "        subj = str(row['subject_id'])\n",
    "        study = str(row['study_id'])\n",
    "        dicom = str(row['dicom_id'])\n",
    "        return f\"p{subj[:2]}/p{subj}/s{study}/{dicom}.jpg\"\n",
    "\n",
    "    df_final['path'] = df_final.apply(get_img_path, axis=1)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 5. ìµœì¢… ì €ì¥ (WBC, BNP í¬í•¨)\n",
    "    # ==============================================================================\n",
    "    final_cols = ['subject_id', 'stay_id', 'study_id', 'path', 'gender', 'anchor_age',\n",
    "                  'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'acuity',\n",
    "                  'WBC', 'BNP'] + target_diseases  # <--- [ì¤‘ìš”] ì—¬ê¸°ì— ì¶”ê°€ë¨!\n",
    "\n",
    "    available_cols = [c for c in final_cols if c in df_final.columns]\n",
    "    df_export = df_final[available_cols]\n",
    "\n",
    "    save_path = SAVE_DIR + 'master_dataset.csv'\n",
    "    df_export.to_csv(save_path, index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ğŸ‰ [ì™„ë£Œ] Master Dataset ìƒì„± ì„±ê³µ! (Lab ë°ì´í„° í¬í•¨)\")\n",
    "    print(f\"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {save_path}\")\n",
    "    print(f\"ğŸ“Š ìµœì¢… ë°ì´í„° ê°œìˆ˜: {len(df_export)}ê°œ\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ì—ëŸ¬ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ì…‹ ì¤‘ ëˆ„ë½ëœ ì´ë¯¸ì§€ë“¤ë„ ìˆê¸° ë•Œë¬¸ì— ëˆ„ë½ëœ ì´ë¯¸ì§€ í–‰ë“¤ì„ ì§€ì›ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ì„¤ì •\n",
    "# ==============================================================================\n",
    "BASE_DIR = '/content/drive/MyDrive/REM_project/github/dataset/split_data/'\n",
    "INPUT_CSV = BASE_DIR + 'master_dataset.csv'\n",
    "OUTPUT_CSV = BASE_DIR + 'master_dataset_verified.csv'\n",
    "\n",
    "# ğŸš¨ ì‹¤ì œ ì´ë¯¸ì§€ê°€ ìˆëŠ” ìµœìƒìœ„ í´ë”\n",
    "IMG_ROOT = '/content/drive/MyDrive/mimic_cxr_data/raw'\n",
    "\n",
    "print(\"ğŸš€ [ê²½ë¡œ ìˆ˜ì • ë° ê²€ì¦] ì‹œì‘...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ë°ì´í„° ë¡œë“œ\n",
    "# ==============================================================================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(f\"ğŸ“‚ ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2.5. [í•µì‹¬ ìˆ˜ì •] ê²½ë¡œ ë¬¸ìì—´ ê³ ì¹˜ê¸° (files/ ì œê±°)\n",
    "# ==============================================================================\n",
    "# ì„¤ëª…: CSVì˜ 'files/p10/...' ë¶€ë¶„ì„ 'p10/...'ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "# ì´ë ‡ê²Œ í•´ì•¼ IMG_ROOTì™€ í•©ì³¤ì„ ë•Œ ê²½ë¡œê°€ ë”± ë§ìŠµë‹ˆë‹¤.\n",
    "print(\"ğŸ”§ ê²½ë¡œ ë¬¸ìì—´ ìˆ˜ì • ì¤‘ ('files/' ì œê±°)...\")\n",
    "df['path'] = df['path'].str.replace('files/', '', regex=False)\n",
    "\n",
    "# í™•ì¸ìš© ì¶œë ¥\n",
    "print(f\"   ğŸ‘‰ ìˆ˜ì • í›„ ê²½ë¡œ ì˜ˆì‹œ: {df['path'].iloc[0]}\")\n",
    "# ì˜ˆìƒ ì¶œë ¥: p10/p10000032/... (files/ê°€ ì—†ì–´ì•¼ í•¨)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ê²€ì¦ í•¨ìˆ˜\n",
    "# ==============================================================================\n",
    "def check_file_exists(row_path):\n",
    "    # ì´ì œ row_pathì—ëŠ” 'files/'ê°€ ì—†ìœ¼ë¯€ë¡œ ë°”ë¡œ í•©ì¹˜ë©´ ë©ë‹ˆë‹¤.\n",
    "    full_path = os.path.join(IMG_ROOT, str(row_path))\n",
    "    return os.path.exists(full_path)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ê³ ì† ë³‘ë ¬ ê²€ì¦\n",
    "# ==============================================================================\n",
    "print(\"   ğŸ•µï¸â€â™‚ï¸ íŒŒì¼ ì‹œìŠ¤í…œ ìŠ¤ìº” ì¤‘...\")\n",
    "paths = df['path'].tolist()\n",
    "\n",
    "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì•ˆì •ì„±ì„ ìœ„í•´ workers=16 ê¶Œì¥\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    results = list(tqdm(executor.map(check_file_exists, paths), total=len(paths), desc=\"Checking Files\"))\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ì €ì¥\n",
    "# ==============================================================================\n",
    "df_verified = df[results].copy()\n",
    "deleted_count = len(df) - len(df_verified)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"âœ… ê²€ì¦ ì™„ë£Œ!\")\n",
    "print(f\"   - ì›ë³¸ ê°œìˆ˜: {len(df)}\")\n",
    "print(f\"   - ìƒì¡´ ê°œìˆ˜: {len(df_verified)}\")\n",
    "print(f\"   - ğŸ‘» ì‚­ì œëœ ìœ ë ¹ ë°ì´í„°: {deleted_count}ê°œ\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if len(df_verified) > 0:\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "    df_verified.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {OUTPUT_CSV}\")\n",
    "    print(\"ğŸ‘‰ ì €ì¥ëœ íŒŒì¼ì€ ê²½ë¡œê°€ 'p10/...' í˜•íƒœë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"ğŸš¨ ì—¬ì „íˆ 0ê°œì…ë‹ˆë‹¤. ê²½ë¡œë¥¼ printí•´ì„œ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë…¼ë¬¸ì— ë„£ê¸° ìœ„í•œ ë°ì´í„° í†µê³„ í‘œë¥¼ ë½‘ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Attempt to unmount first, in case of a problematic previous mount state\n",
    "try:\n",
    "  drive.flush_and_unmount()\n",
    "  print(\"Google Drive unmounted successfully for remount.\")\n",
    "except Exception:\n",
    "  print(\"Google Drive was not mounted or could not be unmounted. Proceeding with mount.\")\n",
    "\n",
    "# Explicitly clear the mount point to avoid 'Mountpoint must not already contain files' error\n",
    "if os.path.exists('/content/drive'):\n",
    "    print(\"Clearing existing /content/drive directory...\")\n",
    "    !rm -rf /content/drive/*\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ë°ì´í„° ë¡œë“œ (ê²½ë¡œ í™•ì¸ í•„ìˆ˜!)\n",
    "# ==============================================================================\n",
    "# 01ë²ˆ íŒŒì¼ì—ì„œ ìƒì„±ëœ ìµœì¢… ë°ì´í„°ì…‹ ê²½ë¡œì…ë‹ˆë‹¤.\n",
    "path = '/content/drive/MyDrive/REM_project/github/dataset/split_data/master_dataset_verified.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ! ì´ {len(df)}ëª…ì˜ í™˜ì ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ì§ˆí™˜ë³„ í†µê³„ (Disease Prevalence)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ğŸ¥ ì§ˆí™˜ë³„ ë°œìƒ ë¹ˆë„ (Prevalence)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "target_diseases = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion', 'Pneumonia']\n",
    "\n",
    "summary_disease = []\n",
    "for disease in target_diseases:\n",
    "    if disease in df.columns:\n",
    "        count = df[disease].sum()\n",
    "        ratio = (count / len(df)) * 100\n",
    "        summary_disease.append([disease, int(count), f\"{ratio:.1f}%\"])\n",
    "\n",
    "df_disease_stats = pd.DataFrame(summary_disease, columns=['Disease', 'Count', 'Percentage'])\n",
    "print(df_disease_stats)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ë°”ì´íƒˆ ë° ì¸êµ¬í†µê³„ (Clinical Characteristics)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ğŸŒ¡ï¸ ì„ìƒì  íŠ¹ì§• (Mean Â± Std)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# ë¶„ì„í•  ì»¬ëŸ¼ ëª©ë¡\n",
    "clinical_cols = ['anchor_age', 'gender', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'WBC', 'BNP']\n",
    "\n",
    "stats_list = []\n",
    "\n",
    "for col in clinical_cols:\n",
    "    if col in df.columns:\n",
    "        # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "        data = df[col]\n",
    "\n",
    "        # [ì£¼ì˜] WBC, BNPëŠ” ê²°ì¸¡ì¹˜ê°€ -1ë¡œ ì±„ì›Œì ¸ ìˆìœ¼ë¯€ë¡œ, -1ì„ ì œì™¸í•˜ê³  í†µê³„ ê³„ì‚°\n",
    "        if col in ['WBC', 'BNP']:\n",
    "            data = data[data != -1]\n",
    "            missing_rate = (1 - len(data)/len(df)) * 100\n",
    "            note = f\"(ê²°ì¸¡ {missing_rate:.1f}%)\"\n",
    "        else:\n",
    "            note = \"\"\n",
    "\n",
    "        # í†µê³„ ê³„ì‚°\n",
    "        mean_val = data.mean()\n",
    "        std_val = data.std()\n",
    "        min_val = data.min()\n",
    "        max_val = data.max()\n",
    "\n",
    "        # ì„±ë³„(Gender)ì€ í‰ê·  ëŒ€ì‹  ë¹„ìœ¨ë¡œ í‘œì‹œ\n",
    "        if col == 'gender':\n",
    "            male_count = data.sum()\n",
    "            male_ratio = data.mean() * 100\n",
    "            stats_list.append([col, f\"Male: {int(male_count)} ({male_ratio:.1f}%)\", \"-\", \"-\", \"-\"])\n",
    "\n",
    "        # ë‚˜ì´(Age)ëŠ” ì •ê·œí™”(Z-score) ë˜ì–´ ìˆìŒì„ ëª…ì‹œ\n",
    "        elif col == 'anchor_age':\n",
    "            stats_list.append([col, f\"{mean_val:.2f} Â± {std_val:.2f}\", f\"{min_val:.2f}\", f\"{max_val:.2f}\", \"Normalized\"])\n",
    "\n",
    "        else:\n",
    "            stats_list.append([col, f\"{mean_val:.1f} Â± {std_val:.1f}\", f\"{min_val:.1f}\", f\"{max_val:.1f}\", note])\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì´ì˜ê²Œ ì¶œë ¥\n",
    "df_stats = pd.DataFrame(stats_list, columns=['Feature', 'Mean Â± Std (or Count)', 'Min', 'Max', 'Note'])\n",
    "print(df_stats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
