{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ë¥¼ ì½”ë© ë¡œì»¬ë¡œ ë¦¬ì‚¬ì´ì¦ˆ ë° ë³µì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ì„¤ì •\n",
    "# ==============================================================================\n",
    "# CSV íŒŒì¼ ê²½ë¡œ (ê²€ì¦ëœ íŒŒì¼ ì‚¬ìš©)\n",
    "CSV_PATH = '/content/drive/MyDrive/REM_project/github/dataset/split_data/master_dataset_verified.csv'\n",
    "\n",
    "# ì›ë³¸ ì´ë¯¸ì§€ê°€ ìˆëŠ” êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê²½ë¡œ\n",
    "DRIVE_IMG_ROOT = '/content/drive/MyDrive/mimic_cxr_data/raw'\n",
    "\n",
    "# ë³µì‚¬í•´ë‘˜ Colab ë¡œì»¬ ê²½ë¡œ (í•™ìŠµí•  ë•Œ ì—¬ê¸°ë¥¼ ë°”ë¼ë³´ê²Œ ë¨)\n",
    "LOCAL_SAVE_DIR = '/content/train_images'\n",
    "\n",
    "# ì´ë¯¸ì§€ í¬ê¸° (DenseNet ì…ë ¥ í¬ê¸°)\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ì¤€ë¹„\n",
    "# ==============================================================================\n",
    "# ì €ì¥í•  í´ë” ìƒì„±\n",
    "os.makedirs(LOCAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"ğŸš€ ì´ {len(df)}ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ë¡œì»¬ë¡œ ë¦¬ì‚¬ì´ì¦ˆ ë° ë³µì‚¬í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ë³€í™˜ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)\n",
    "# ==============================================================================\n",
    "def process_and_save_image(row_path):\n",
    "    try:\n",
    "        # 1. ì›ë³¸ ê²½ë¡œ (ë“œë¼ì´ë¸Œ)\n",
    "        full_src_path = os.path.join(DRIVE_IMG_ROOT, str(row_path))\n",
    "\n",
    "        # 2. ì €ì¥í•  íŒŒì¼ëª… (íŒŒì¼ëª…ë§Œ ë–¼ì–´ëƒ„)\n",
    "        filename = os.path.basename(str(row_path))\n",
    "        # í˜¹ì‹œ í™•ì¥ìê°€ ì—†ê±°ë‚˜ ë‹¤ë¥´ë©´ jpgë¡œ í†µì¼\n",
    "        filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "\n",
    "        full_dst_path = os.path.join(LOCAL_SAVE_DIR, filename)\n",
    "\n",
    "        # 3. ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆëœ€ (ì¬ì‹¤í–‰ ì‹œ ì‹œê°„ ì ˆì•½)\n",
    "        if os.path.exists(full_dst_path):\n",
    "            return True\n",
    "\n",
    "        # 4. ì´ë¯¸ì§€ ì—´ê¸° ë° ë¦¬ì‚¬ì´ì¦ˆ\n",
    "        with Image.open(full_src_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            img = img.resize(IMG_SIZE, Image.LANCZOS)\n",
    "            # 5. ë¡œì»¬ì— ì €ì¥ (ì••ì¶•ë¥  85 ì •ë„ë©´ ì¶©ë¶„)\n",
    "            img.save(full_dst_path, \"JPEG\", quality=85)\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # ì—ëŸ¬ ë‚˜ë©´ ê·¸ëƒ¥ ë„˜ì–´ê° (ë‚˜ì¤‘ì— í™•ì¸ ê°€ëŠ¥)\n",
    "        return False\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ì‹¤í–‰ (ê³ ì† ë³‘ë ¬ ì²˜ë¦¬)\n",
    "# ==============================================================================\n",
    "paths = df['path'].tolist()\n",
    "\n",
    "# max_workers=16 ì •ë„ë¡œ ë³‘ë ¬ ì²˜ë¦¬\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    results = list(tqdm(executor.map(process_and_save_image, paths), total=len(paths), desc=\"Moving & Resizing\"))\n",
    "\n",
    "success_count = sum(results)\n",
    "print(\"-\" * 50)\n",
    "print(f\"âœ… ì‘ì—… ì™„ë£Œ!\")\n",
    "print(f\"   - ì„±ê³µ: {success_count}ì¥\")\n",
    "print(f\"   - ì €ì¥ ìœ„ì¹˜: {LOCAL_SAVE_DIR}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ\n",
    "# ==============================================================================\n",
    "BASE_DIR = '/content/drive/MyDrive/REM_project/github/dataset/split_data/'\n",
    "DATA_PATH = BASE_DIR + 'master_dataset_verified.csv'\n",
    "IMG_ROOT = '/content/train_images/'\n",
    "\n",
    "print(\"ğŸ“‚ ë°ì´í„° ë¡œë“œ ë° ë¶„í•  ì¤‘...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "subject_ids = df['subject_id'].unique()\n",
    "\n",
    "# Train / Val / Test ë¶„í• \n",
    "train_ids, temp_ids = train_test_split(subject_ids, test_size=0.2, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = df[df['subject_id'].isin(train_ids)].reset_index(drop=True)\n",
    "val_df = df[df['subject_id'].isin(val_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"   - Train: {len(train_df)}ê°œ\")\n",
    "print(f\"   - Val: {len(val_df)}ê°œ\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Tabular ë°ì´í„° ì„¤ì •\n",
    "# ==============================================================================\n",
    "TABULAR_COLS = [\n",
    "    'gender', 'anchor_age', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp',\n",
    "    'WBC', 'BNP'\n",
    "]\n",
    "\n",
    "scaler_mean = train_df[TABULAR_COLS].mean()\n",
    "scaler_std = train_df[TABULAR_COLS].std().replace(0, 1)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Dataset í´ë˜ìŠ¤\n",
    "# ==============================================================================\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_root, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "        self.target_cols = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion', 'Pneumonia']\n",
    "        self.tabular_cols = TABULAR_COLS\n",
    "        self.mean = scaler_mean\n",
    "        self.std = scaler_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        original_path = str(row['path'])\n",
    "        filename = os.path.basename(original_path)\n",
    "        filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "        full_img_path = os.path.join(self.img_root, filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # ë°”ì´íƒˆ ë°ì´í„°\n",
    "        tabular = row[self.tabular_cols].values.astype(np.float32)\n",
    "        tabular = (tabular - self.mean.values.astype(np.float32)) / self.std.values.astype(np.float32)\n",
    "\n",
    "        # ë¼ë²¨\n",
    "        labels = row[self.target_cols].values.astype(np.float32)\n",
    "\n",
    "        return image, torch.tensor(tabular), torch.tensor(labels)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. DataLoader (â˜… ì¤‘ìš”: ì˜ë£Œ ë°ì´í„°ì— ì•ˆì „í•œ ì¦ê°• ì ìš©)\n",
    "# ==============================================================================\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# [ìˆ˜ì •ë¨] í‰ë¶€ ì—‘ìŠ¤ë ˆì´ì— ì•ˆì „í•œ ë³€í™˜ë§Œ ì ìš©\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # 1. íšŒì „: í™˜ìê°€ ì‚ë”±í•˜ê²Œ ëˆ„ì›Œìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ -10~10ë„ ì •ë„ëŠ” ê´œì°®ìŒ\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "\n",
    "    # 2. ì´ë™ ë° ìŠ¤ì¼€ì¼: ì´¬ì˜ ìœ„ì¹˜ê°€ ì¡°ê¸ˆ ë‹¤ë¥´ê±°ë‚˜ ì¤Œì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ (ì´ë™ 5%, ì¶•ì†Œ/í™•ëŒ€ 10%)\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.1)),\n",
    "\n",
    "    # 3. í…ì„œ ë³€í™˜ ë° ì •ê·œí™”\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std)\n",
    "])\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ëŠ” ì›ë³¸ ìœ ì§€\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std)\n",
    "])\n",
    "\n",
    "train_dataset = MultimodalDataset(train_df, IMG_ROOT, train_transform)\n",
    "val_dataset = MultimodalDataset(val_df, IMG_ROOT, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"âœ… ì•ˆì „í•œ ì¦ê°• ì ìš© ì™„ë£Œ! Train Loader ë°°ì¹˜ ê°œìˆ˜: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# 1. Gated Cross Attention (ê¸°ì¡´ ìœ ì§€)\n",
    "class GatedCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model=1024):\n",
    "        super(GatedCrossAttention, self).__init__()\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.gate = nn.Parameter(torch.tensor(0.0))\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, img_feat, tab_feat):\n",
    "        B, C, H, W = img_feat.shape\n",
    "        img_flat = img_feat.view(B, C, -1).permute(0, 2, 1)\n",
    "        vital_query = tab_feat.unsqueeze(1)\n",
    "\n",
    "        Q = self.query_proj(vital_query)\n",
    "        K = self.key_proj(img_flat)\n",
    "        V = self.value_proj(img_flat)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (C ** 0.5)\n",
    "        attn_map = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_map, V)\n",
    "\n",
    "        gate_value = torch.sigmoid(self.gate)\n",
    "        fused_feat = vital_query + (gate_value * context)\n",
    "        fused_feat = self.norm(self.out_proj(fused_feat))\n",
    "\n",
    "        return fused_feat.squeeze(1), attn_map.view(B, H, W)\n",
    "\n",
    "# 2. ì „ì²´ ëª¨ë¸ ì¡°ë¦½ (Dropout ê°•í™” + ì€ë‹‰ì¸µ ì¶”ê°€)\n",
    "class DenseNetFusionModel(nn.Module):\n",
    "    def __init__(self, num_tabular_features, num_classes=1):\n",
    "        super(DenseNetFusionModel, self).__init__()\n",
    "\n",
    "        # (1) ì´ë¯¸ì§€ ì¸ì½”ë”\n",
    "        self.img_enc = models.densenet121(weights='DEFAULT')\n",
    "        self.img_enc_features = self.img_enc.features\n",
    "\n",
    "        # (2) ë°”ì´íƒˆ ì¸ì½”ë” (Dropout ì¶”ê°€)\n",
    "        self.tab_enc = nn.Sequential(\n",
    "            nn.Linear(num_tabular_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),      # ê³¼ì í•© ë°©ì§€\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)       # ê³¼ì í•© ë°©ì§€\n",
    "        )\n",
    "\n",
    "        # (3) Fusion\n",
    "        self.fusion = GatedCrossAttention(d_model=1024)\n",
    "\n",
    "        # (4) â˜… ìˆ˜ì •ëœ ë¶„ë¥˜ê¸° (ì€ë‹‰ì¸µ 2ê°œ ì¶”ê°€) â˜…\n",
    "        # êµ¬ì¡°: 1024 -> [512] -> [256] -> 6\n",
    "        self.classifier = nn.Sequential(\n",
    "            # ì€ë‹‰ì¸µ 1\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),     # ê°•ë ¥í•œ Dropout\n",
    "\n",
    "            # ì€ë‹‰ì¸µ 2\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),     # ê°•ë ¥í•œ Dropout\n",
    "\n",
    "            # ì¶œë ¥ì¸µ\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_img, x_tab):\n",
    "        img_feat = self.img_enc_features(x_img)\n",
    "        tab_feat = self.tab_enc(x_tab)\n",
    "\n",
    "        fused_feat, attn_map = self.fusion(img_feat, tab_feat)\n",
    "        logits = self.classifier(fused_feat)\n",
    "\n",
    "        return logits, attn_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ ì‹œì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 6\n",
    "TABULAR_DIM = 10\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = DenseNetFusionModel(num_classes=NUM_CLASSES, num_tabular_features=TABULAR_DIM)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3) # L2 ê·œì œ ìœ ì§€\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2)\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for images, tabular, labels in loop:\n",
    "        images = images.to(device).float()\n",
    "        tabular = tabular.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(images, tabular)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        loop.set_description(f\"Train Loss: {loss.item():.4f}\")\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "# ê²€ì¦ í•¨ìˆ˜\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_targets, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, tabular, labels in loader:\n",
    "            images = images.to(device).float()\n",
    "            tabular = tabular.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            outputs, _ = model(images, tabular)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_targets, all_preds, average='macro')\n",
    "    except:\n",
    "        auroc = 0.5\n",
    "    return epoch_loss, auroc\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰\n",
    "best_auroc = 0.0\n",
    "print(\"ğŸš€ Re-Training (Safe Augmentation + Dropout 0.5 + 2 Hidden Layers)...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_auroc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    scheduler.step(val_auroc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUROC: {val_auroc:.4f}\")\n",
    "\n",
    "    if val_auroc > best_auroc:\n",
    "        best_auroc = val_auroc\n",
    "        torch.save(model.state_dict(), os.path.join(BASE_DIR, \"best_fusion_model_v3.pth\"))\n",
    "        print(f\"âœ… Best Model Saved! (AUROC: {val_auroc:.4f})\")\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ì˜¨ë¦¬ ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ImageOnlyDenseNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        # í“¨ì „ ëª¨ë¸ê³¼ ë™ì¼í•œ ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
    "        self.backbone = models.densenet121(weights='DEFAULT')\n",
    "\n",
    "        # DenseNetì˜ ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸° ì¸µ(1024 -> num_classes)ì„\n",
    "        # í“¨ì „ ëª¨ë¸ê³¼ ë˜‘ê°™ì€ \"ê°•ë ¥í•œ ë¶„ë¥˜ê¸°\"ë¡œ êµì²´\n",
    "        num_ftrs = self.backbone.classifier.in_features # 1024\n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            # ì€ë‹‰ì¸µ 1\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # í“¨ì „ ëª¨ë¸ê³¼ ë™ì¼\n",
    "\n",
    "            # ì€ë‹‰ì¸µ 2\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # í“¨ì „ ëª¨ë¸ê³¼ ë™ì¼\n",
    "\n",
    "            # ì¶œë ¥ì¸µ\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë¯¸ì§€ ì˜¨ë¦¬ í•™ìŠµ ì‹œì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ì„¤ì • (í“¨ì „ ëª¨ë¸ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 6\n",
    "BASE_DIR = '/content/drive/MyDrive/REM_project/github/dataset/split_data/'\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model_img = ImageOnlyDenseNet(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model_img.parameters(), lr=LEARNING_RATE, weight_decay=1e-3) # L2 ê·œì œ ë™ì¼ ì ìš©\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2)\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜ (Tabular ë°ì´í„°ëŠ” ë°›ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
    "def train_image_only(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(loader, leave=True, desc=\"Train Image-Only\")\n",
    "\n",
    "    # Loaderì—ì„œ tabular ë°ì´í„°ë„ ë‚˜ì˜¤ì§€ë§Œ ë¬´ì‹œ(_)\n",
    "    for images, _, labels in loop:\n",
    "        images = images.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) # ì´ë¯¸ì§€ë§Œ ì…ë ¥\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        loop.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "# ê²€ì¦ í•¨ìˆ˜\n",
    "def validate_image_only(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, labels in loader:\n",
    "            images = images.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_targets, all_preds, average='macro')\n",
    "    except:\n",
    "        auroc = 0.5\n",
    "\n",
    "    return running_loss / len(loader.dataset), auroc\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰\n",
    "best_auroc = 0.0\n",
    "print(\"ğŸš€ Image-Only Baseline Re-Training (Safe Aug + Dropout 0.5)...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_image_only(model_img, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_auroc = validate_image_only(model_img, val_loader, criterion, device)\n",
    "\n",
    "    scheduler.step(val_auroc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUROC: {val_auroc:.4f}\")\n",
    "\n",
    "    if val_auroc > best_auroc:\n",
    "        best_auroc = val_auroc\n",
    "        # íŒŒì¼ëª… ë‹¤ë¥´ê²Œ ì €ì¥\n",
    "        torch.save(model_img.state_dict(), os.path.join(BASE_DIR, \"best_image_only_model_v2.pth\"))\n",
    "        print(f\"âœ… Best Baseline Saved! (AUROC: {val_auroc:.4f})\")\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
